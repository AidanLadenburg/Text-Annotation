{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o86egNsX9yMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cba7a8b-f2be-4027-8393-2d9643ba8fd5"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from IPython.display import HTML\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "import pdb\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, merge, Dropout, Dense, Input, Flatten\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "categories = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc']\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, categories=categories)\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bFB5ti993Ul"
      },
      "source": [
        "texts = []\n",
        "labels = newsgroups_train.target\n",
        "texts = newsgroups_train.data\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "x_train = data\n",
        "y_train = labels\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPKwSXRN9-Hc"
      },
      "source": [
        "GLOVE_DIR = \"C:\\\\Users\\\\pythonrunfiles\"\n",
        "word_index = tokenizer.word_index\n",
        "embeddings_index = {}\n",
        "f = open('/content/drive/My Drive/Colab Notebooks/Data/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "l_cov1 = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)\n",
        "l_pool2 = MaxPooling1D(5)(l_cov2)\n",
        "l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)\n",
        "l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling\n",
        "l_flat = Flatten()(l_pool3)\n",
        "l_dense = Dense(128, activation='relu')(l_flat)\n",
        "preds = Dense(19, activation='softmax')(l_dense)\n",
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1MCqopj-A40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d5c3e5-3d10-4240-d5dd-8efaffc76db7"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1000)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 1000, 100)         13187400  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 996, 128)          64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 199, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 195, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 39, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 35, 128)           82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 1, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 19)                2451      \n",
            "=================================================================\n",
            "Total params: 13,434,587\n",
            "Trainable params: 13,434,587\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDQl_oB2-COt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a056df7d-0c2a-4eeb-8b99-eb7f010442d7"
      },
      "source": [
        "model.fit(x_train, y_train, validation_split= 0.2, epochs=20, batch_size=128)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "55/55 [==============================] - 13s 93ms/step - loss: 2.9808 - acc: 0.0739 - val_loss: 2.5750 - val_acc: 0.1429\n",
            "Epoch 2/20\n",
            "55/55 [==============================] - 5s 86ms/step - loss: 2.3167 - acc: 0.2277 - val_loss: 1.8056 - val_acc: 0.3720\n",
            "Epoch 3/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 1.6273 - acc: 0.4299 - val_loss: 1.5352 - val_acc: 0.4554\n",
            "Epoch 4/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 1.2055 - acc: 0.5774 - val_loss: 1.2548 - val_acc: 0.5971\n",
            "Epoch 5/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.9243 - acc: 0.6715 - val_loss: 1.3368 - val_acc: 0.5629\n",
            "Epoch 6/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.6676 - acc: 0.7836 - val_loss: 0.7888 - val_acc: 0.7377\n",
            "Epoch 7/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.4934 - acc: 0.8365 - val_loss: 0.7220 - val_acc: 0.7737\n",
            "Epoch 8/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.3547 - acc: 0.8840 - val_loss: 0.6494 - val_acc: 0.8017\n",
            "Epoch 9/20\n",
            "55/55 [==============================] - 5s 84ms/step - loss: 0.2560 - acc: 0.9214 - val_loss: 0.7253 - val_acc: 0.7909\n",
            "Epoch 10/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.2170 - acc: 0.9302 - val_loss: 0.6300 - val_acc: 0.8257\n",
            "Epoch 11/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.1542 - acc: 0.9593 - val_loss: 0.6755 - val_acc: 0.8166\n",
            "Epoch 12/20\n",
            "55/55 [==============================] - 5s 84ms/step - loss: 0.1097 - acc: 0.9737 - val_loss: 0.6145 - val_acc: 0.8491\n",
            "Epoch 13/20\n",
            "55/55 [==============================] - 5s 83ms/step - loss: 0.0646 - acc: 0.9840 - val_loss: 1.9169 - val_acc: 0.6840\n",
            "Epoch 14/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.1236 - acc: 0.9661 - val_loss: 0.6902 - val_acc: 0.8440\n",
            "Epoch 15/20\n",
            "55/55 [==============================] - 5s 85ms/step - loss: 0.0550 - acc: 0.9895 - val_loss: 1.8808 - val_acc: 0.7497\n",
            "Epoch 16/20\n",
            "55/55 [==============================] - 5s 84ms/step - loss: 0.1928 - acc: 0.9635 - val_loss: 0.6996 - val_acc: 0.8566\n",
            "Epoch 17/20\n",
            "55/55 [==============================] - 5s 84ms/step - loss: 0.0542 - acc: 0.9886 - val_loss: 0.9483 - val_acc: 0.8166\n",
            "Epoch 18/20\n",
            "55/55 [==============================] - 5s 84ms/step - loss: 0.0190 - acc: 0.9962 - val_loss: 0.7724 - val_acc: 0.8577\n",
            "Epoch 19/20\n",
            "55/55 [==============================] - 5s 84ms/step - loss: 0.0202 - acc: 0.9952 - val_loss: 0.7596 - val_acc: 0.8606\n",
            "Epoch 20/20\n",
            "55/55 [==============================] - 5s 84ms/step - loss: 0.0178 - acc: 0.9961 - val_loss: 0.7972 - val_acc: 0.8583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbf8d119f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvIPj3QJ-Pds"
      },
      "source": [
        "#@title ## Auto Annotations!\n",
        "#@markdown Try-it-out by pasting or typing your text below:\n",
        "#X = df.blind_assesment.values\n",
        "Type_here = \"There are several major rules, which differ only slightly. The \\\"Official Baseball Rules\\\" govern all professional play in the United States and Canada, including the World Baseball Classic. The complete rules are published as the Official Baseball Rules at MLB.com, the official web site of Major League Baseball in the United States and Canada. The rules are also published in book form in North America by the Sporting News. Many amateur and youth leagues use the \\\"OBR\\\" with only a few modifications for safety.  Other slightly modified versions of the \\\"Official Baseball Rules\\\" are used in competitions operated by the International Baseball Federation (IBAF), including the Olympics and the World Cup, most leagues outside North America (such as Japan's Nippon Professional Baseball), and youth baseball organizations such as Little League, PONY League, and Cal Ripken League.  The baseball rulebook of the National Collegiate Athletic Association (NCAA), aside from governing the games of that organization's members, is also used by several other competitions involving college-aged players.  U.S. high school and high school-age baseball is governed by the rules of the National Federation of State High School Associations (NFHS).\"#@param {type:\"string\"}\n",
        "maxlen = 1000\n",
        "tokenizer = Tokenizer(num_words= 400000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences([Type_here])\n",
        "data = pad_sequences(sequences,padding='post', maxlen=maxlen)\n",
        "indices = np.arange(data.shape[0])\n",
        "data = data[indices]\n",
        "Xtst = data\n",
        "\n",
        "y_pred = model.predict(Xtst)\n",
        "\n",
        "class_idx = np.argmax(y_pred[0])\n",
        "class_output = model.output[:, class_idx]\n",
        "last_conv_layer = model.get_layer(\"conv1d\")\n",
        "\n",
        "grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
        "pooled_grads = K.mean(grads)\n",
        "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "pooled_grads_value, conv_layer_output_value = iterate([Xtst])\n",
        "\n",
        "    \n",
        "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "heatmap = np.maximum(heatmap,0)\n",
        "heatmap /= np.max(heatmap)#normalise values in the prediction\n",
        "norm_len = maxlen/last_conv_layer.output_shape[1] # find the ratio of the text vs the conv layer length\n",
        "\n",
        "html = \"\"\n",
        "\n",
        "html += \"<span><h3>Based on the description, the model believes that this is {}.\".format(categories[int((np.argmax(y_pred)))])\n",
        "html += \"<small></small></h3></span>\".format(abs(((y_pred[0][0]*100)-50)*2))\n",
        "for j,i in enumerate(tokenizer.sequences_to_texts(Xtst)[0].split()):\n",
        "  html += \"<span style='background-color:rgba({},0,50,{})'>{} </span>\".format(heatmap[math.floor(j/norm_len)]*255,heatmap[math.floor(j/norm_len)]-0.3,i)\n",
        "\n",
        "HTML(html)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}